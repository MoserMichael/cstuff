---
layout: post
title: Semantic nets and social networks
---

{{ page.title }}
----------------

<p class="publish_date">
18 July 2011

</p>
Man has named child after [a Button](http://www.bbc.co.uk/news/world-middle-east-13417930)
(At least it is not [this button](http://en.wikipedia.org/wiki/Big_red_button#Symbolism) )

While staying on the subject of Facebook, I wonder if you can automate creation of [semantic nets](http://en.wikipedia.org/wiki/Semantic_network)) by doing statistical analysis of things that people do 'like'. In order to do that you would need information kept by both google and facebook; Facebook knows what you 'like' (corresponding to the concepts of a semantic net) Google knows what you have searched for and how you got it (the edges between concepts) / the edges are extracted from query words. Nodes are concepts that you 'liked'. Ups now google [has that too](https://plus.google.com)

This sort of research would be similar to what they are doing in Natural Language Processing, these days it's all statistics and [n-grams](http://en.wikipedia.org/wiki/N-gram) With such an encompassing knowledge system the resulting Knowledge Monopoly â„¢ could serve you content aware adds on future thoughts even before you have thought of them.

.. Now there was this recent science fiction idea of the [Singularity event](http://www.amazon.com/dp/0670033847/) being [near](http://www.fourmilab.ch/fourmilog/archives/2011-02/001293.html) - It says that a machine capable of thought will blow away all current technological limitations and we will all shoot out for the stars, and all that should happen Really soon.

Well I don't know, even if the machine comes up with a grand plan, it will depend on the political will of Humans to implement it; so I guess that all the bots will end up sending emails to each other being largely ignored ... Just like us Humans ;-) also we might end up with a [really depressed Marvin](http://en.wikipedia.org/wiki/Marvin_the_Paranoid_Android)

### How to test the intelligence of machines

With the aim of furthering the Singularity event - I would now like to propose a better test that should tell if a machine is intelligent or not.

Normally a program dies when its environment changes in some unexpected ways (this is also called [Software rot](http://en.wikipedia.org/wiki/Software_rot) ); I would say that a program (or any other technical system) is intelligent, if it can adapt all by itself to unexpected changes of its environment or adjust to unexpected changes in its input. By that definition a search engines that produces good results for any queries makes an intelligent system.

Interestingly now I remember that the book [The Ghost in the Machine](http://en.wikipedia.org/wiki/The_Ghost_in_the_Machine) by [Arthur Koestler](http://en.wikipedia.org/wiki/Arthur_Koestler) also mentions the ability of the higher nervous system to compensate for damages in other parts of the body / psyche as a defining feature of intelligent systems. Somehow I think this happens to be a very Jewish definition of intelligence - wit and intelligence had to compensate for the very disadvantageous situation of discriminations / persecutions / deliberate prescriptions of what you may or may not do for a living. That makes the Wall-E test a very tough test.

Somehow defining intelligence as the ability of a machine to convince with small talk may look like a stereotypically English way of looking at things, probably all these definitions of intelligence mirror some cultural preferences of the person who is framing this definition, in other words, Intelligence is defined by culture X by what culture X values most and what culture X would like the machine to do. So while a definition may be ok for most of the time, still no single definition will ever define the whole experience of being alive / being intelligent.

'What is Man, what does it mean to be Human' - the famous [riddle of the mythical Sphinx](http://en.wikipedia.org/wiki/Sphinx#The_Riddle_of_the_Sphinx) is now the question that the Robot asks. That question makes sense for a Sphinx - it is half man half animal, similar to the Robot. Some say that's the reason why we [will not actually build any Robots](http://zompist.wordpress.com/2011/06/24/no-singularity-for-me-mater/) , who wants a complex beast like this around? Funny, are the ancient Egyptian, Mesopotamian/Babylonian/Persian, Indian sphinxes asking any existential questions, or is it just it just the Classical Greek copy-sphinx?

Back to our two definitions of intelligence - of course both definitions are equally dumb; both try to define Intelligence as a [black box](http://en.wikipedia.org/wiki/Black_box) - try do define the requirements for a intelligent system in terms of the input and expected output only. So in psychology behaviorism falls short when trying to explain things like learning and language, and here we are back to zero ...

I have now invested some time to understand [this course on machine learning](http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=MachineLearning); so far I am not impressed, it's more of the same parlor tricks. Wikipedia [says here](http://en.wikipedia.org/wiki/Machine_learning) - "There are many similarities between machine learning theory and statistical inference, although they use different terms." , I tend to agree with that.

### Again on the singularity

Another sphinx like aspect of the machine: with each new stage new questions come up, more questions than answers. What if they manage to build intelligent machines but those fail to be original thinkers? What if real creativity can't be automated?

That, of course would remind me of the following scene:
<iframe width="560" height="315" src="//www.youtube.com/embed/3MyZTPPgsZA" frameborder="0" allowfullscreen></iframe>
